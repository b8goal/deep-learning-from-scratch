##### 날짜 : 2024-03-18 14:50
##### 주제 : #Perceptron #Neuron #Rosenblatt
---
# Memo :
>[!note]

## Neuron
우리 사람의 뇌는 신경계를 구성하는 주된 세포인 뉴런(neuron)을 약 1000억개 정도 가지고 있으며, 뉴런들은 시냅스라는 구조를 통해 전기, 화학적 신호를 주고 받음으로써 다양한 정보를 받아들이고, 그 정보를 저장하는 기능을 수행한다.

아래 그림은 하나의 뉴런에서 신호를 입력 받고 그에 대한 결과 신호를 출력하는 개념을 도식화 한 것이다.
![](https://i.imgur.com/HJTkTcf.png)
우리 사람의 뇌속에는 위 그림과 같은 **뉴런이 1000억개 가까이 서로가 다층적으로 복잡하게 연결**되어 있다. 이런 뇌 구조로 뇌활동이 일어나는 것이며, 인간이 수행하는 모든 일(생각하고, 배우고, 창조하고, 기억하고, 느끼고,, 아파하고,, 등등 모든 것)에 관여하고 있다.

1943년 신경과학자인 Warren S. McCulloch과 논리학자인 Walter Pitts는 하나의 사람 뇌 신경세포를 하나의 이진(Binary)출력을 가지는 단순 논리 게이트로 설명했다.

위 그림에서 여러 개의 입력 신호가 가지돌기(Dendrite)에 도착하면 신경세포 내에서 이들을 하나의 신호로 통합하고, 통합된 신호 값이 어떤 임계값을 초과하면 하나의 단일 신호가 생성되며, 이 신호가 축삭돌기(Axon)를 통해 다른 신경세포로 전달하는 것으로 이해했습니다. 이렇게 단순화 된 원리로 동작하는 뇌 세포를 McCulloch-Pitts뉴런(MCP 뉴런)이라 부른다.

1957년 코넬 항공 연구소에 근무하던 Frank Rosenblatt은 MCP 뉴런 모델을 기초로 **퍼셉트론(Perceptron) 학습 규칙**이라는 개념을 고안하게 되는데, Rosenblatt은 하나의 MCP 뉴런이 출력신호를 발생할지 안할지 결정하기 위해, MCP 뉴런으로 들어오는 각 입력값에 곱해지는 가중치 값을 자동적으로 학습하는 알고리즘을 제안했다.

​이 알고리즘은 머신러닝의 지도학습이나 분류(classification)의 맥락에서 볼 때, 하나의 샘플이 어떤 클래스에 속해 있는지 예측하는데 사용될 수 있다.

아래 그림은 Rosenblatt이 제안한 퍼셉트론 알고리즘 개념을 도식화 한 것이다.
![퍼셉트론 알고리즘 개념을 도식화](https://i.imgur.com/wAGssBX.png)

여기서 **_x0~xn_** 은 퍼셉트론 알고리즘으로 입력되는 값이며, **_w0~wn_** 은 각각 **_x0~xn_** 에 곱해지는 가중치이다. 입력값은 보통 분류를 위한 데이터의 특성(feature)을 나타내는 값으로 이루어져 있으며, 이 특성값 **_x0~xn_** 에 가중치 **_w0~wn_** 을 곱한 값을 모두 더하여 하나의 값으로 만든다. 이 값을 만드는 함수를 **순입력 함수(net input 함수)** 라고 부른다. 순입력 함수의 결과값을 특정 임계값과 비교를 하고, 순입력 함수 결과값이 이 임계값보다 크면 1, 그렇지 않으면 -1로 출력하는 함수를 정의한다. 이 함수를 **활성 함수(Activation function)** 라고 부른다.

퍼셉트론은 다수의 트레이닝 데이터를 이용하여 일종의 지도 학습을 수행하는 알고리즘이다. 트레이닝 데이터에는 데이터의 특성값에 대응되는 실제 결과값을 가지고 있어야 한다. 입력되는 특성값 **_x0~xn_** 에 대한 실제 결과값을 y라고 한다면 이 y를 활성 함수에 의해 -1 또는 1로 변환한다. 이렇게 변환한 값과 퍼셉트론 알고리즘에 의해 예측된 값이 다르면 이 두 개의 값이 같아질 때까지 특정식에 의해 가중치 **_w0~wn_** 을 업데이트 한다.

위 그림을 보다 단순하게 도식화하면 다음 그림과 같이 입력층, 중간층, 출력층과 같이 나타낼 수 있다.
![](https://i.imgur.com/yTSk4RC.png)
여기서 중간층을 노드 또는 뉴런이라 부르며, 입력층은 다른 노드의 출력값이 입력값으로 전달되는 층이며, 출력층은 이 노드의 출력값이 다른 노드로 전달되는 층이라고 생각하면 된다.

이와 같이 중간층이 하나의 노드로 구성되어 중간층과 출력층의 구분이 없는 구조를 단순 또는 단층 퍼셉트론이라 부르며, 중간층을 구성하는 노드가 여러 개이고, 이러한 중간층이 다수로 구성되어 있는 구조를 다층 퍼셉트론이라 부른다.

아래는 다층 퍼셉트론을 응용한 인공신경망 구조의 한 예시입니다.
![](https://i.imgur.com/ogOmkPi.png)
이러한 다층 인공신경망을 학습하는 알고리즘을 딥 러닝(Deep Learning)이라고 말한다.

이것이 퍼셉트론의 핵심이다. 그러면 좀 더 자세하게 살펴보도록 한다.

MCP 뉴런과 Rosenblatt의 퍼셉트론 모델은 사람 뇌의 단일 뉴런이 작동하는 방법을 흉내내기 위해 환원 접근법(redcutionist approach)을 이용한다. 이는 초기 가중치를 임의의 값으로 정의하고 예측값의 활성 함수 리턴값과 실제 결과값의 활성 함수 리턴값이 동일하게 나올 때까지 가중치의 값을 계속 수정하는 방법이다.

Rosenblatt의 초기 퍼셉트론 알고리즘을 요약하면 다음과 같다.

1. 입력되는 특성값에 곱해지는 가중치 **_w_** 값들을 모두 0 또는 작은 값으로 무작위 할당함
2. 임계값을 정의함(보통 0으로 정의합니다.)
3. 트레이닝 데이터 샘플 **_x_** 를 순입력 함수를 이용해 가중치와 각각 곱한 후 그 총합을 구함
4. 활성 함수를 이용해 트레이닝 데이터 샘플에 대한 예측값을 -1 또는 1로 결과가 나오게 함
5. 트레이닝 데이터 샘플의 실제 결과값에 대한 활성 함수 리턴값과 4에서 나온 예측값을 비교함
6. 예측값과 결과값이 다르면 모든 가중치 **_w_** 를 업데이트하고 3의 과정부터 다시 시작함
7. 예측값과 결과값이 동일하게 나오면 패스

자, 아래와 같이 **_n_** 개의 특성값을 가진 **_m_** 개의 트레이닝 데이터가 있다고 가정한다.
![](https://i.imgur.com/buqTj6n.png)

이 트레이닝 데이터로 퍼셉트론 알고리즘을 이용해 머신러닝을 수행하려고 한다. Rosenblatt의 퍼셉트론 알고리즘에서 **_x0_** 는 실제 트레이닝 데이터의 특성값과는 무관하며 **_x0_** 의 초기값으로 보통 1로 둡니다. 퍼셉트론 알고리즘에서 **_x0_** 를 바이어스(bias)라 부른다. 따라서 실제 트레이닝 데이터의 특성값은 **_x1_** 부터 시작한다고 보면 된다.

이제 우리가 1로 정한 **_x0_** 의 값과 **_n_** 개의 트레이닝 데이터의 각 특성값 **_x1~xn_** 과 이 특성값에 곱할 가중치 **_w1~wn_** 의 값을 0과 1사이의 임의의 값으로 랜덤하게 할당한 후 위 표를 아래와 같이 다시 구성한다.
![](https://i.imgur.com/TM5js62.png)

레이닝 데이터1~**_m_** 까지 각 특성값에 대해 퍼셉트론 알고리즘을 구동한다. 퍼셉트론 알고리즘에서 가중치를 업데이트 하는 식은 다음과 같이 정의된다.

![](https://blogfiles.pstatic.net/MjAxNzAzMDRfMjMw/MDAxNDg4NjAyNzA3OTQ1.x2jNmZwbFAUC1nRfSbUfGiLJuJY8G4XhnB9SmyKFtYcg.YRrgjWsEoX0rrkwDmpuNA89yn-DUKGGsXHc047HCeT0g.PNG.samsjang/%EC%BA%A1%EC%B2%983.PNG?type=w2)

여기서 **_wj_** 는 트레이닝 데이터의 **_j_** 번째 특성값 **_xj_** 와 곱하는 가중치이며, **_y_** 는 트레이닝 데이터의 실제 결과값에 대한 활성 함수 리턴값, **_y^_** 은 예측값에 대한 활성 함수 리턴값입니다. **(_y - y^_)** 앞에 곱한 그리스 문자 **_η_** 는 학습률(learning rate)이라 하며, Rosenblatt의 퍼셉트론에서는 learning rate을 매우 작은 값으로 할당합니다.

퍼셉트론 알고리즘은 다음과 같은 방식으로 구동합니다.

1. **_​w0_**~**_wn_** 의 값을 0.0, 임계값 0.0, 에타값은 0.1로 둡니다.
2. 트레이닝 데이터1의 특성값들에 대한 예측값의 활성 함수 리턴값을 계산한다.
3. 2에서 계산된 값이 이 실제 결과값의 활성 함수 리턴값과 같으면 가중치 업데이트 없이 다음 트레이닝 데이터에 대해 2번 과정으로 넘어간다.
4. 예측값의 활성 함수 리턴값이 실제 결과값의 활성 함수 리턴값과 다르면 위 식에 의해 **_w0_**~**_wn_** 의 가중치를 업데이트 한다. 
5. 트레이닝 데이터2~**_m_** 에 대해서 2~4를 반복한다.

위와 같은 절차로 트레이닝 데이터1~**_m_** 까지 모든 예측값이 실제 결과값과 동일해질 때까지 반복합니다. 트레이닝 데이터1~**_m_** 까지 예측값에 대한 활성 함수 리턴값이 실제 결과값의 활성 함수 리턴값과 동일하면 퍼셉트론 학습은 종료된다.
## Summary
>[!summary]
- 뉴런의 입출력 신호의 개념을 도식화하여  하나의 사람 뇌 신경세포를 하나의 이진(Binary)출력을 가지는 단순 논리 게이트로 설명했다.
- 이런 MCP 뉴런 모델을 기초로 Perceptron 학습 규칙이라는 개념이 고안됨.
- Perceptron을 multi-layer로 설계하고 이를 학습하면 deep-learning이 될 수 있음. 
# Reference
---
### 출처(참고문헌)
- [[2편] 퍼셉트론(Perceptron) - 인공신경망의 기초개념 : 네이버 블로그 (naver.com)](https://blog.naver.com/PostView.naver?blogId=samsjang&logNo=220948258166&parentCategoryNo=&categoryNo=87&viewDate=&isShowPopularPosts=false&from=postView)
### 연결문서
- [[2.1 Perceptron Concept]]