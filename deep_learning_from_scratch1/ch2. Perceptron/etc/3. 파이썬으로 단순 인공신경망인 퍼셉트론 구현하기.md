##### 날짜 : 2024-03-18 16:31
##### 주제 : #Perceptron #implementation
---
# Memo :
>[!note]

먼저, 논리곱 AND 연산에 대해 퍼셉트론 알고리즘으로 머신러닝을 수행하는 예를 보면서 이해를 해보도록 합니다.

참을 1, 거짓을 0으로 정의할 때, 논리곱 AND 연산은 다음과 같이 정의됩니다.

0 AND 0 = 0
0 AND 1 = 0
1 AND 0 = 0
1 AND 1 = 1

퍼셉트론 알고리즘을 이용해 AND 연산을 학습시키는 것이 우리의 목표입니다.

AND 연산을 위해 필요한 트레이닝 데이터는 위에서 보인 4가지이며, 트레이닝 데이터의 특성값은 0과 1 2개입니다. 이를 이전 포스팅에서 정의한 표를 이용해 나타내보면 다음과 같습니다.

![](https://i.imgur.com/SvOMbfL.png)

퍼셉트론 알고리즘에서 보통 0 또는 매우 작은 값을 가중치의 초기값으로 둔다고 했는데, 우리의 알고리즘에서는 초기 가중치의 값을 모두 0으로 둡니다.

learning rate η는 0.1로, 임계값은 0으로 정의하고, 순입력 함수 결과값이 임계값 0보다 크면 1, 그렇지 않으면 -1로 정의해봅니다.

![](https://i.imgur.com/IpBhYzg.png)

자, 이전 포스팅에서 소개된 알고리즘 원리에 따라 퍼셉트론 학습을 수행해봅니다.

1. 트레이닝 데이터1에 대해 순입력 함수 리턴값을 계산합니다.
w0 x x0 + w1 x x1 + w2 x x2 = 0.0 x1 + 0.0 x 0 + 0.0 x 0 = 0

2. 순입력 함수의 리턴값과 임계값 0을 비교하면 0보다 크지 않으므로 활성 함수 리턴값은 -1이 됩니다. 이는 실제 결과값에 대한 활성 함수 리턴값과 일치하므로 가중치 업데이트 없이 다음 트레이닝 데이터 2로 넘어갑니다.

3. 트레이닝 데이터2~3까지는 예측된 값의 활성 함수 리턴값과 실제 결과값의 활성 함수 리턴값이 일치하므로 가중치 업데이트 없이 트레이닝 데이터4로 넘어갑니다.

4. 트레이닝 데이터4에 대해 1과 같이 계산한 퍼셉트론 예측값의 활성 함수 리턴값은 -1로 나왔지만 실제 결과값의 활성 함수 리턴값은 1이므로 예측값과 결과값이 다릅니다. 따라서 아래의 식에 의해 가중치를 다시 계산합니다.

w0 = w0 + 0.1(1-(-1))1 = 0 + 0.2 = 0.2
w1 = w1 + 0.1(1-(-1))1 = 0 + 0.2 = 0.2
w2 = w2 + 0.1(1-(-1))1 = 0 + 0.2 = 0.2

5. 트레이닝 데이터4에서 가중치가 업데이트 되었으므로 새롭게 업데이트 된 가중치를 가지고 트레이닝 데이터1부터 1의 과정을 다시 수행합니다.

이런 식으로 트레이닝 데이터1~4에 대한 예측값의 활성 함수 리턴값과 실제 결과값의 활성 함수 리턴값이 같아지도록 가중치를 계속 업데이트해보면
w0 = -0.4, w1 = 0.4, w2 = 0.2 일 때, 모든 트레이닝 데이터에 대한 예측값과 결과값의 활성 함수 리턴값이 같아집니다.

이와 같은 퍼셉트론 알고리즘은 다음과 같은 파이썬 코드로 구현된된다. 참고로 퍼셉트론 알고리즘을 구현한 아래의 코드는 Sebastian Raschka의 'Python Machine Learning'책에 있는 코드이다. 아래의 코드를 작성하고 perceptron.py로 저장한다.

### perceptron.py
```python
import numpy as np

class Perceptron():
    def __init__(self, thresholds=0.0, eta=0.01, n_iter=10):
        self.thresholds = thresholds
        self.eta = eta
        self.n_iter = n_iter

    def fit(self, X, y):
        self.w_ = np.zeros(1+X.shape[1])
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target-self.predict(xi))
                self.w_[1:] += update * xi
                self.w_[0]  += update
                errors += int(update != 0.0)
            self.errors_.append(errors)
            print(self.w_)

        return self

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        return np.where(self.net_input(X) > self.thresholds, 1, -1)

if __name__ == '__main__':
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([-1, -1, -1, 1])

ppn = Perceptron(eta=0.1)
ppn.fit(X,y)
print(ppn.errors_)
```
#### output
w0 = -0.4, w1 = 0.4, w2 = 0.2 일 때, 모든 트레이닝 데이터에 대한 예측값과 결과값의 활성 함수 리턴값이 같아지는 것을 확인했다.
```bash
[0.2 0.2 0.2]
[0.  0.4 0.2]
[-0.2  0.4  0.2]
[-0.2  0.4  0.4]
[-0.4  0.4  0.2]
[-0.4  0.4  0.2]
[-0.4  0.4  0.2]
[-0.4  0.4  0.2]
[-0.4  0.4  0.2]
[-0.4  0.4  0.2]
[1, 3, 3, 2, 1, 0, 0, 0, 0, 0]
```
## Summary
>[!summary]
- 머신러닝의 기초적인 알고리즘 중 하나인 퍼셉트론을 이용하여 논리곱 AND 연산을 학습하는 과정을 설명
- 학습 과정에서는 순입력 함수의 결과값을 기반으로 활성화 함수의 리턴값을 계산하고, 이를 실제 결과값과 비교하여 가중치를 업데이트한다.
- 트레이닝 데이터를 통해 반복 학습을 수행한 결과, 최종적으로 모든 트레이닝 데이터에 대한 예측값이 실제 결과값과 일치하게된다.
- 이 과정을 통해 퍼셉트론 알고리즘이 어떻게 논리 연산을 학습할 수 있는지를 보여줌.
# Reference
---
### 출처(참고문헌)
- [[3편] 파이썬으로 단순 인공신경망인 퍼셉트론 구현하기 : 네이버 블로그 (naver.com)](https://blog.naver.com/PostView.naver?blogId=samsjang&logNo=220951769938&parentCategoryNo=&categoryNo=87&viewDate=&isShowPopularPosts=false&from=postView)
### 연결문서
- [[2. 퍼셉트론(Perceptron) - 인공신경망의 기초개념]]